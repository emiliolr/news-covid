{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from time import process_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['nytimes', 'CNN', 'bbcworld', 'theeconomist', 'reuters', 'WSJ', 'TIME', 'ABC', 'washingtonpost', 'AP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get a string back to a datetime object\n",
    "to_dt = lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turns a string of a list of tuples back into a list of tuples ('loft')\n",
    "def str_to_loft(loft):\n",
    "    stack = []\n",
    "    rebuilt = []\n",
    "\n",
    "    for thing in loft.split(','):\n",
    "        stack.append(thing.translate(str.maketrans('', '', '\\'\"[]()')).strip())\n",
    "        if ')' in thing:\n",
    "            rebuilt.append(tuple([stack.pop(), int(stack.pop()), stack.pop()][ : : -1]))\n",
    "            \n",
    "    return rebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some standard cleaning steps (removing text that isn't actualy part of the tweet)\n",
    "def clean_text(text):\n",
    "    clean = text.replace('…', '')\n",
    "    if clean.startswith('RT @'):\n",
    "        clean = ':'.join(clean.split(':')[1 : ])\n",
    "        \n",
    "    return clean.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from github\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags = re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying NER:** The way it's written here, SA is applied before NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that returns the detected entities in a text\n",
    "#output is a tuple with format: (ENTITY TEXT, ENTITY FREQ, ENTITY LABEL)\n",
    "def get_ents(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    ent_dict = {}\n",
    "    for ent in doc.ents:\n",
    "        exclude = ['coronavirus', 'covid', 'covid-19']\n",
    "        if ('@' not in ent.text) and ('#' not in ent.text) and (ent.text.lower() not in exclude):\n",
    "            last_check = True\n",
    "            if (ent.start_char - 1) > 0:\n",
    "                if text[ent.start_char - 1] in ['@', '#']:\n",
    "                    last_check = False\n",
    "            if last_check:\n",
    "                if ent.text in ent_dict:\n",
    "                    ent_dict[ent.text][0] += 1\n",
    "                else:\n",
    "                    ent_dict[ent.text] = [1, ent.label_]\n",
    "                \n",
    "    return [(x, ent_dict[x][0], ent_dict[x][1]) for x in ent_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nytimes\n",
      "CNN\n",
      "bbcworld\n",
      "theeconomist\n",
      "reuters\n",
      "WSJ\n",
      "TIME\n",
      "ABC\n",
      "washingtonpost\n",
      "AP\n"
     ]
    }
   ],
   "source": [
    "#applying NER to each data set\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "for s in sources:\n",
    "    data = pd.read_csv('DATA_W_ANAlYSIS/' + s + '_SENT.csv', index_col = 'ID')\n",
    "    app_col = data['Tweet_Text'].fillna('').apply(remove_emoji).apply(clean_text)\n",
    "    data['Named_Entities'] = app_col.apply(lambda x: get_ents(x, nlp))\n",
    "    data.to_csv('DATA_W_ANAlYSIS/' + s + '_FINAL.csv')\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nytimes:\n",
      "True\n",
      "True\n",
      "0\n",
      "0\n",
      "\n",
      "CNN:\n",
      "True\n",
      "True\n",
      "0\n",
      "0\n",
      "\n",
      "bbcworld:\n",
      "True\n",
      "True\n",
      "0\n",
      "0\n",
      "\n",
      "theeconomist:\n",
      "True\n",
      "True\n",
      "0\n",
      "0\n",
      "\n",
      "reuters:\n",
      "True\n",
      "True\n",
      "0\n",
      "0\n",
      "\n",
      "WSJ:\n",
      "True\n",
      "True\n",
      "0\n",
      "0\n",
      "\n",
      "TIME:\n",
      "True\n",
      "True\n",
      "0\n",
      "0\n",
      "\n",
      "ABC:\n",
      "True\n",
      "True\n",
      "0\n",
      "0\n",
      "\n",
      "washingtonpost:\n",
      "True\n",
      "True\n",
      "0\n",
      "0\n",
      "\n",
      "AP:\n",
      "True\n",
      "True\n",
      "0\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking some things for NER application\n",
    "for s in sources:\n",
    "    test = pd.read_csv('DATA_W_ANAlYSIS/' + s + '_FINAL.csv', index_col = 'ID')\n",
    "    other = pd.read_csv('CUT_DATA/' + s + '.csv')\n",
    "    test['Date_Time'] = test['Date_Time'].apply(to_dt)\n",
    "    test['Named_Entities'] = test['Named_Entities'].apply(str_to_loft)\n",
    "    print(s + ':')\n",
    "    #lengths of data sets match\n",
    "    print(len(test) == len(other)) #should be True\n",
    "    #all elements of the NEs column are lists\n",
    "    print(test['Named_Entities'].apply(lambda x: type(x) == list).sum() == len(test)) #should be True\n",
    "    #there are no null values in NE column\n",
    "    print(test['Named_Entities'].isnull().sum()) #should be 0\n",
    "    #there are no null values in SA column\n",
    "    print(test['Sentiment'].isnull().sum()) #should be 0\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying Sentiment Analysis:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying sentiment analysis to each data set\n",
    "for s in sources:\n",
    "    data = pd.read_csv('CUT_DATA/' + s + '.csv', index_col = 'ID')\n",
    "    app_col = data['Tweet_Text'].fillna('').apply(clean_text)\n",
    "    data['Sentiment'] = app_col.apply(lambda x: TextBlob(x).sentiment.polarity).apply(lambda x: round(x, 3))\n",
    "    data.to_csv('DATA_W_ANAlYSIS/' + s + '_SENT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nytimes 0\n",
      "CNN 0\n",
      "bbcworld 0\n",
      "theeconomist 0\n",
      "reuters 0\n",
      "WSJ 0\n",
      "TIME 0\n",
      "ABC 0\n",
      "washingtonpost 0\n",
      "AP 0\n"
     ]
    }
   ],
   "source": [
    "#checking the application of SA\n",
    "for s in sources:\n",
    "    data = pd.read_csv('DATA_W_ANAlYSIS/' + s + '_SENT.csv', index_col = 'ID')\n",
    "    #there are no null values in SA column\n",
    "    print(s, data['Sentiment'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Establishing a Word Collocations Pipeline:** This isn't used in the paper but could be a good method for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('coronavirus', 'update'), ('coronavirus', 'outbreak'), ('coronavirus', 'crisis'), ('coronavirus', 'fears'), ('coronavirus', 'pandemic'), ('coronavirus', 'cases'), ('coronavirus', 'deaths'), ('global', 'coronavirus'), ('coronavirus', 'death'), ('latest', 'coronavirus'), ('amid', 'coronavirus'), ('coronavirus', 'lockdown'), ('italy', 'coronavirus')]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('CUT_DATA/bbcworld.csv', index_col = 'ID')\n",
    "\n",
    "#cleaning and putting in list (normalizing with '.lower()')\n",
    "data['Clean_Text'] = data['Tweet_Text'].apply(clean_text)\n",
    "list_of_vals = [x.lower() for x in list(data['Clean_Text'].fillna('').values)]\n",
    "\n",
    "#joining the tweets together and applying SWT\n",
    "words = ' ~ '.join(list_of_vals)\n",
    "tok_words = word_tokenize(words)\n",
    "\n",
    "#removing punctuation, which could stop contiguous words from being associated\n",
    "#NOT including ',.:;~?!', these are end-of-line characters and should keep tokens seperate\n",
    "punct = [p for p in string.punctuation if p not in ',.:;~?!'] + ['’', \"'s\", '``', '“', '”', '—', \"n't\"]\n",
    "no_punct = [w for w in tok_words if w not in punct]\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(no_punct)\n",
    "\n",
    "#taking out low frequency bigrams\n",
    "#if not using a word filter, would be good to bump up freq filter\n",
    "finder.apply_freq_filter(5)\n",
    "\n",
    "#taking out bigrams with stops\n",
    "stops = stopwords.words('English') + [p for p in string.punctuation] + ['’', \"'s\", '``', '“', '”', '—', \"n't\"]\n",
    "stops_fil = lambda w: w in stops\n",
    "finder.apply_word_filter(stops_fil)\n",
    "\n",
    "#looking for bigrams containing a specific word\n",
    "word_fil = lambda *w: 'coronavirus' not in w\n",
    "finder.apply_ngram_filter(word_fil)\n",
    "\n",
    "#ranking by PMI\n",
    "#MAYBE use several association measures here!\n",
    "print(finder.nbest(BigramAssocMeasures.pmi, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
